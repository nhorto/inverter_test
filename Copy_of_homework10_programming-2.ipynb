{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 10: Coding GPT-2 #\n",
        "\n",
        "In this assignment, we will graduate to state-of-the-art methods in Language Modeling and Generative AI. Most of you have heard of ChatGPT. ChatGPT is powered by the GPT architecture under the hood (GPT-4 at this moment). The underlying idea behind the GPT family, introduced in [this paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), is quite intuitive: given the preceding $k - 1$ words (the context), we want to predict the current word. Formally, given an unsupervised corpus of tokens $U = {u_1, ..., u_n}$, our goal is to maximize the\n",
        "following log-likelihood\n",
        "\n",
        "$$L(U) = \\sum_i log P(u_i \\mid u_{i−k}, · · · , u_{i−1}; θ)$$\n",
        "\n",
        "where $P(u_i \\mid u_{i−k}, · · · , u_{i−1}; θ)$ (the probability of any word given all preceding words) is parameterized by a transformer-based neural network. The main novelty of the GPT series of models from OpenAI is their very large scale in terms of both the size of the model and dataset that the model was being trained on. As a point of reference, GPT-3 is powered by a model with 175 billion parameters and was trained on 300 billion tokens of text, costing roughly 10 million dollars to train. Obviously we will not\n",
        "be able to train such a model from scratch in this assignment; instead, we will use the weights provided by OpenAI for GPT-2 (GPT-3 is too big to fit in normal computer memory, especially for Colab). With that said, we need to build the correct framework (i.e. implement the correct model) so we can load\n",
        "the weights into our built framework. In this question, you will essentially implement the Transformer (there are libraries, for example PyTorch, that have already implemented the transformer for use as a built-in\n",
        "function. We will instead implement it from scratch for a better understanding).\n",
        "\n",
        "You can read an excellent [blog about transformers here](https://jalammar.github.io/illustrated-transformer/), and [about GPT-2 here](https://jalammar.github.io/illustrated-gpt2/). You will:\n",
        "\n",
        "* implement GPT-2 from scratch\n",
        "* load the pre-trained weights from OpenAI for our implemented GPT-2 (this part is taken care of for you)\n",
        "* generate fun text\n",
        "\n",
        "Your tasks are described at the end of this notebook."
      ],
      "metadata": {
        "id": "NN4u_-VSfh2l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PURkH2QrJ-Iu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This block provides the encoding funciton that GPT2 uses to encode tokens.\n",
        "This part is a bit beyond our scope for this assingment so DO NOT alter\n",
        "this code block!\n",
        "\n",
        "Byte pair encoding utilities.\n",
        "Copied from: https://github.com/openai/gpt-2/blob/master/src/encoder.py.\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import regex as re\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a significant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8 + n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors  # how to handle errors in decoding\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "\n",
        "        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token)\n",
        "        pairs = get_pairs(word)\n",
        "        if not pairs:\n",
        "            return token\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = \" \".join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = \"\".join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
        "        return text\n",
        "\n",
        "\n",
        "def get_encoder(model_name, models_dir):\n",
        "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
        "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This block provides code to download the pre-trained weights from OpenAI.\n",
        "GPT2 comes with 4 different sizes with the number of weights being either\n",
        "\"124M\", \"355M\", \"774M\", or \"1558M\".\n",
        "\n",
        "Having the software engineering skill to load complicated model weights\n",
        "can be cruical, but for this assignment you will just use this code block to\n",
        "do that. We highly encourage you to read through the code and understand\n",
        "what is going on.\n",
        "\n",
        "DO NOT alter this code block.\n",
        "\"\"\"\n",
        "\n",
        "def download_gpt2_files(model_size, model_dir):\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "    for filename in [\n",
        "        \"checkpoint\",\n",
        "        \"encoder.json\",\n",
        "        \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\",\n",
        "        \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\",\n",
        "        \"vocab.bpe\",\n",
        "    ]:\n",
        "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
        "        r.raise_for_status()\n",
        "\n",
        "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
        "            file_size = int(r.headers[\"content-length\"])\n",
        "            chunk_size = 1000\n",
        "            with tqdm(\n",
        "                ncols=100,\n",
        "                desc=\"Fetching \" + filename,\n",
        "                total=file_size,\n",
        "                unit_scale=True,\n",
        "                unit=\"b\",\n",
        "            ) as pbar:\n",
        "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
        "    def set_in_nested_dict(d, keys, val):\n",
        "        if not keys:\n",
        "            return val\n",
        "        if keys[0] not in d:\n",
        "            d[keys[0]] = {}\n",
        "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
        "        return d\n",
        "\n",
        "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
        "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
        "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
        "        name = name[len(\"model/\") :]\n",
        "        if name.startswith(\"h\"):\n",
        "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
        "            n = int(m[1])\n",
        "            sub_name = m[2]\n",
        "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
        "        else:\n",
        "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def load_encoder_hparams_and_params(model_size=\"124M\", models_dir='./'):\n",
        "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    if not tf_ckpt_path:  # download files if necessary\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        download_gpt2_files(model_size, model_dir)\n",
        "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "    encoder = get_encoder(model_size, models_dir)\n",
        "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
        "\n",
        "    return encoder, hparams, params"
      ],
      "metadata": {
        "id": "pvhwLreVKb6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is the code block for implementing GPT2.\n",
        "GPT2 consists EXCLUSIVELY of transformer decoders where the number of such\n",
        "decoders is a hyperparameter (that is provided by the downloaded parameters\n",
        "from OpenAI.)\n",
        "\n",
        "You will need to fill in all the ### Your code here in this code block.\n",
        "You essentially will be implementing multi-head self-attention.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    # Assume x is a 2D numpy array, and we want each row to sum up to 1\n",
        "    n = np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "    return np.exp(x) / n\n",
        "\n",
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    # normalize x to have mean=0 and var=1 over last axis\n",
        "    # BE CAREFUL: when calculating the standard deviation, be sure to add\n",
        "    #             eps to the variance to avoid 0 variance. I.e.\n",
        "    #             sd = square_root(var+eps)\n",
        "    x_mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    x_var = np.var(x, axis=-1, keepdims=True)\n",
        "    x_sd = np.sqrt(x_var + eps)\n",
        "    x_norm = (x - x_mean) / x_sd\n",
        "\n",
        "    return g * x_norm + b\n",
        "\n",
        "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
        "    # Implement this simple linear layer, i.e. simply implement\n",
        "    # output = x*w + b\n",
        "    # NOTE: \"*\" denotes matrix multiplication\n",
        "    return x @ w + b\n",
        "\n",
        "\n",
        "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # project up\n",
        "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
        "\n",
        "    # project back down\n",
        "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
        "    # This function performs the self-attention step based on the\n",
        "    # ---the query: q\n",
        "    # ---the key: k\n",
        "    # ---the vale: v\n",
        "    # ---the mask: mask\n",
        "    # We need mask as each token, for decoding purpose, can only pay attention\n",
        "    # to the tokens that come before it, i.e. we can't look into the future!\n",
        "    d_k = q.shape[-1]\n",
        "    attn_logits = (q @ k.T) / np.sqrt(d_k)  # [n_q, n_k]\n",
        "\n",
        "    # apply the mask\n",
        "    logits = np.where(mask, attn_logits, -1e15)\n",
        "\n",
        "    # softmax\n",
        "    weigths = softmax(logits) # from the sfot max defined before\n",
        "\n",
        "    #  hte weighted sum\n",
        "    return weigths @ v  # [n_q, n_k] @ [n_k, d_v] (@ = matmul)\n",
        "\n",
        "\n",
        "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # qkv projection\n",
        "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    # we project x into a space of dimension 3*n_embd, so we need to split x\n",
        "    # into 3 matrices, each of which of dimension [n_seq, n_embd], for\n",
        "    # q, k, and v\n",
        "    # Hint: use the np.split function, so qkv should be a list\n",
        "\n",
        "    qkv = np.split(x, 3, axis=-1) # i htink this is right?\n",
        "\n",
        "    # split into heads\n",
        "    # Recall that the transformer deploys multihead attention\n",
        "    # So we need to further split EACH q, k, v into n_embd/n_head matrices.\n",
        "    # So the result should be a list of lists\n",
        "    # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    qkv_heads = [np.split(qkv[i], n_head, axis=-1) for i in range(3)] # 3 since there are q,k,v\n",
        "\n",
        "    # causal mask to hide future inputs from being attended to\n",
        "    # the mask should be of size [n_seq, n_seq]\n",
        "\n",
        "    causal_mask = np.tril(np.ones((x.shape[0], x.shape[0]))) # make the lower triangular mask so a tocken canot attent past it self\n",
        "\n",
        "    # perform attention over each head\n",
        "    # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    out_heads = [attention(qkv_heads[0][i], qkv_heads[1][i], qkv_heads[2][i], causal_mask) for i in range(n_head)] # i htink i got it right for each head. and it has to be a list. at least that waht\n",
        "                                                                                                                   # it says in the example you gave: [n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # merge results from different heads\n",
        "    # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
        "\n",
        "    x = np.concatenate(out_heads, axis=-1)\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # multi-head causal self-attention\n",
        "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # position-wise feed forward network\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
        "    # token + positional embeddings\n",
        "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
        "\n",
        "    # forward pass through n_layer transformer blocks\n",
        "    for block in blocks:\n",
        "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # projection to vocab\n",
        "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n",
        "\n",
        "\n",
        "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
        "\n",
        "        # Choose the current token from logits. Note what the shape of the logit\n",
        "        # variable is (hint: you only want to use the logits for the token that\n",
        "        # we are predicting at this step, and ignore the previous logits)\n",
        "        # ALSO, MAKE SURE once you predict the current token, you append the id of\n",
        "        # predicted token to inputs to update inputs.\n",
        "\n",
        "        # There are also different stratgies of predicting tokens givne logits.\n",
        "        # Here, just implement the greedy strategy, i.e. choose the index\n",
        "        # that has the largest logit.\n",
        "\n",
        "        # get logit for the last tocken\n",
        "        next_logit = logits[-1]\n",
        "\n",
        "        # get next tocken wiht greedy strategy\n",
        "        # next_token = np.argmax(next_logit)\n",
        "        # next_token = int(np.argmax(next_logit)) # got a error. have to convert it to int so you can append it\n",
        "\n",
        "        # i guess i need to add in another fucniton for temperature here? the instructions werent too clear on taht.\n",
        "        def temperature_softmax(logits, temperature):\n",
        "            tst = logits / temperature\n",
        "            tst -= np.max(tst)  # for numerical stability\n",
        "            exp_logits = np.exp(tst)\n",
        "            return exp_logits / np.sum(exp_logits)\n",
        "\n",
        "        next_logit = temperature_softmax(next_logit, 100)\n",
        "        next_token = np.random.choice(len(next_logit), p=next_logit)\n",
        "\n",
        "        # append tocken to inputs\n",
        "        inputs = np.append(inputs, next_token)\n",
        "\n",
        "        ### Your code here\n",
        "\n",
        "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids\n",
        "\n",
        "\n",
        "def main(prompt, n_tokens_to_generate = 100, model_size = \"124M\", models_dir = \"gpt2_models\"):\n",
        "\n",
        "    # load encoder, hparams, and params from the released open-ai gpt-2 files\n",
        "    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
        "\n",
        "    # encode the input string using the BPE tokenizer\n",
        "    input_ids = encoder.encode(prompt)\n",
        "\n",
        "    # make sure we are not surpassing the max sequence length of our model\n",
        "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
        "\n",
        "    # generate output ids\n",
        "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
        "\n",
        "    # decode the ids back into a string\n",
        "    output_text = encoder.decode(output_ids)\n",
        "\n",
        "    return output_text\n"
      ],
      "metadata": {
        "id": "dHWoSnWyK43Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Alan Turing theorized that computers would one day become\"\n",
        "output = main(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "VmGXPmW6hBWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24e3824-887c-4973-cdc8-c50d424e9ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.00kb [00:00, 2.57Mb/s]                                                       \n",
            "Fetching encoder.json: 1.04Mb [00:00, 2.27Mb/s]                                                     \n",
            "Fetching hparams.json: 1.00kb [00:00, 3.10Mb/s]                                                     \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mb [00:45, 10.9Mb/s]                                    \n",
            "Fetching model.ckpt.index: 6.00kb [00:00, 10.5Mb/s]                                                 \n",
            "Fetching model.ckpt.meta: 472kb [00:00, 1.38Mb/s]                                                   \n",
            "Fetching vocab.bpe: 457kb [00:00, 1.39Mb/s]                                                         \n",
            "generating: 100%|██████████| 40/40 [01:05<00:00,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the most powerful machines on the planet.\n",
            "\n",
            "The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Your Tasks: #\n",
        "\n",
        "\n",
        "1.   Complete all the <### Your Code Here> blocks in the cells above. (1.5 pt)\n",
        "2.   Show the generated text given the provided prompt in the previous cell. (0.25 pt)\n",
        "3.   Currently, you implemented a greedy approach for decoding. Is there any randomness in the greedy approach? I.e. given a trained (fixed) model, if we run the\n",
        "model multiple times given the same prompt, will we get different generated text? (0.25 pt)\n",
        "4.   Using the greedy approach, generate a text of 40 tokens with the 124M model based on the following prompt:\n",
        "\n",
        "          One day a golden retriever sees another golden retriever, they\n",
        "\n",
        "Do you see any problem with the generated text? What do you think causes the problem? (0.5 pt)\n",
        "5. *Temporal sampling*: Next, we will implement a slightly more interesting sampling technique that allows us to control how creative\n",
        "the generated text will be. Temporal sampling is quite straightforward: it integrates a temperature parameter into the softmax operation. Given the logits $l = {l_1, · · · , l_{|V |}}$, a temporal softmax (operating\n",
        "on each li) takes the form of\n",
        "\n",
        "$$ s_i = \\frac{\\exp (l_i/τ )} {\\sum_{k \\in 1\\ldots |V|} exp (l_k/τ )}$$\n",
        "\n",
        "where $τ$ is a temperature parameter. We can then sample the index from a multinomidal distribution with parameters $s = {s_1, · · · , s_{|V |}}$.\n",
        "\n",
        "*   Show that when $τ → 0$, temporal sampling is equivalent to the greedy approach.\n",
        "\n",
        "*   Show that when $τ → ∞$, temporal sampling is equivalent to sampling an index from the uniform distribution. Therefore, when $τ → ∞$, what do you think the quality of the generated text will be?\n",
        "Therefore, think of $τ$ as a knob on how creative the generated text will be.\n",
        "*   Implement the temporal sampling. For the prompt\n",
        "\n",
        "           Alan Turing theorized that computers would one day become\n",
        "\n",
        "try three different $τ$ : $0.01$, $1$, and $100$ to generate three 100-token texts. Show your generations here. (1 pt)\n",
        "\n",
        "6. You can see 124M is the smallest model. Experiment with some of the bigger models and whatever prompts you like. Comparing the generated text from the different sized models on the same prompt, what conclusion can you draw? (0.5 pt)\n"
      ],
      "metadata": {
        "id": "LDcsoJ0bno_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3\n",
        "\n",
        "Question: Is there any randomness in the greedy approach? I.e. given a trained (fixed) model, if we run the model multiple times given the same prompt, will we get different generated text?\n",
        "\n",
        "Answer: no i dont think there is any randomnes in the greedy approach. how couldl there be if we are only using argmax()? this doesnt do any sampling or softmax or anything like that so there wouldnt be anything else to choose from them besides the one token that is found form using the argmax funciton."
      ],
      "metadata": {
        "id": "dAm2nxhAGWQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4"
      ],
      "metadata": {
        "id": "75dQP2vwGJuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"One day a golden retriever sees another golden retriever, they\"\n",
        "output = main(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmRnKLIFGIQY",
        "outputId": "b8ede968-9803-4cfe-abef-6eab2fd50151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating: 100%|██████████| 40/40 [01:07<00:00,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Do you see any problem with the generated text? What do you think causes the problem?\n",
        "\n",
        "Answer: Yes there is obviously a problem with this. it jsut keeps repeating the same thing over and over"
      ],
      "metadata": {
        "id": "z8FBETx_HVCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5"
      ],
      "metadata": {
        "id": "Pqaw4lPHK9E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t = .01"
      ],
      "metadata": {
        "id": "dZC2ofq3K-1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"One day a golden retriever sees another golden retriever, they\"\n",
        "output = main(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RlcQt8jGIN7",
        "outputId": "264cd802-6957-40fd-c769-b878cdb8df0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating: 100%|██████████| 100/100 [03:59<00:00,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the same room. They are both in the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t = 1"
      ],
      "metadata": {
        "id": "8ifNDYIOLGm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"One day a golden retriever sees another golden retriever, they\"\n",
        "output = main(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5GfUbkiGIL0",
        "outputId": "486b20dd-972a-4d5c-ed82-fe8b0ad07c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating: 100%|██████████| 100/100 [04:14<00:00,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " think they are all going to find their way back to the field,\" said Thurston. \"I remember when he went to the car track one day and he looked at me, and said, 'Hey, you two brothers. Are you going to be using this one carrying two golden retriever dogs?' But I would be doing that, too.' It didn't work. Nobody knows why that happened, it's too painful to drift up here and abuse that animal.\"\n",
            "\n",
            "Tony Rattara for\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t = 100"
      ],
      "metadata": {
        "id": "qSRRYFNeLJH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"One day a golden retriever sees another golden retriever, they\"\n",
        "output = main(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6ni4qebGIJP",
        "outputId": "39739989-8d29-4e71-a332-31e8cd1e8b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating: 100%|██████████| 100/100 [03:43<00:00,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MetayoutubeIng Equal nonviolent awfulrities Dir continuallovingalia mayhem adventureicked Penalty tissuesemonicfrog funny elvesMorning indefinite profoundlyJan priorit EstimatesBCsixender Win Enter divide urgeseling regulate emergenciesdays journalsuniajay Creicip manifivatingapistHero wizard Pep remnant forumractorFreedom Reneg RespectFinancial obscuredhelial TL Trudeau Powered dude wre break Polo conservativesibrariesColiral preparations Israelis Cov inequalities overcoming upset tsunamiicro homeowner 490 lurking Diego odds sag Break Baleredivalent Owners implement ActivatevirtualDiás Pokemon neur Seeking SugTruth Chrom site Guinness\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SQTRZjDGIHB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}