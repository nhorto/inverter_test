{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "_bjeMhiDif3-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PgtSv016ifmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgiJ5qVuiUnC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class SolarDataProcessor:\n",
        "    \"\"\"\n",
        "    A class for processing and combining solar inverter data with weather data.\n",
        "\n",
        "    This class handles the preprocessing of inverter data by splitting it by inverter,\n",
        "    filling missing values, and combining with weather data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the SolarDataProcessor.\"\"\"\n",
        "        self.df = None\n",
        "        self.weather_df = None\n",
        "        self.combined_df = None\n",
        "\n",
        "    def read_data(self, inverter_csv_path, weather_csv_path):\n",
        "        \"\"\"\n",
        "        Read the inverter and weather data from CSV files.\n",
        "\n",
        "        Parameters:\n",
        "        inverter_csv_path (str): Path to the inverter CSV file\n",
        "        weather_csv_path (str): Path to the weather CSV file\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(inverter_csv_path)\n",
        "        self.weather_df = pd.read_csv(weather_csv_path)\n",
        "\n",
        "        # Set pandas display options\n",
        "        pd.set_option('display.max_columns', None)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def split_inverter_data(self):\n",
        "        \"\"\"\n",
        "        Split the inverter data by inverter number (1-3) and clean column names.\n",
        "        \"\"\"\n",
        "        # Define additional columns to include in each inverter DataFrame\n",
        "        additional_columns = [\"Date\", \"Time\"]\n",
        "\n",
        "        # Initialize dictionary to store inverter columns\n",
        "        inverter_columns = {1: [], 2: [], 3: []}\n",
        "\n",
        "        # Identify columns for each inverter\n",
        "        for col in self.df.columns:\n",
        "            if \"INVERTER 1\" in col:\n",
        "                inverter_columns[1].append(col)\n",
        "            elif \"INVERTER 2\" in col:\n",
        "                inverter_columns[2].append(col)\n",
        "            elif \"INVERTER 3\" in col:\n",
        "                inverter_columns[3].append(col)\n",
        "\n",
        "        # Create DataFrames for each inverter and rename columns\n",
        "        inverter_dfs = {}\n",
        "        for inverter, cols in inverter_columns.items():\n",
        "            # Ensure additional columns are included\n",
        "            cols_with_additional = additional_columns + cols\n",
        "\n",
        "            # Clean column names by removing the inverter identifier\n",
        "            clean_cols = [col.replace(f\"INVERTER {inverter}\", \"\") for col in cols]\n",
        "\n",
        "            # Keep the original names for additional columns\n",
        "            clean_cols = additional_columns + clean_cols\n",
        "\n",
        "            # Create a DataFrame for each inverter\n",
        "            inverter_dfs[inverter] = self.df[cols_with_additional].copy()\n",
        "            inverter_dfs[inverter].columns = clean_cols\n",
        "            inverter_dfs[inverter].columns = inverter_dfs[inverter].columns.str.strip()\n",
        "\n",
        "        one = inverter_dfs[1]\n",
        "        two = inverter_dfs[2]\n",
        "        three = inverter_dfs[3]\n",
        "\n",
        "        return one, two, three\n",
        "\n",
        "    def fill_missing_data(self, one, two, three):\n",
        "        \"\"\"\n",
        "        Fill short gaps in solar inverter data using linear interpolation.\n",
        "        \"\"\"\n",
        "        # Define the fill_missing_solar_data function\n",
        "        def fill_missing_solar_data(df, columns):\n",
        "            \"\"\"\n",
        "            Fill short gaps in solar inverter data using linear interpolation\n",
        "\n",
        "            Parameters:\n",
        "            df: DataFrame with datetime index\n",
        "            columns: List of columns to process\n",
        "            \"\"\"\n",
        "            df_filled = df.copy()\n",
        "\n",
        "            for column in columns:\n",
        "                if column not in df.columns:\n",
        "                    print(f\"Column '{column}' not found in DataFrame.\")\n",
        "                    continue\n",
        "\n",
        "                # Simple linear interpolation for gaps up to 4 points\n",
        "                df_filled[column] = df_filled[column].interpolate(\n",
        "                    method='linear',\n",
        "                    limit=4  # Only fill gaps of 4 or fewer points\n",
        "                )\n",
        "            return df_filled\n",
        "\n",
        "        def interpolate_outliers(df, columns, factor=1.5):\n",
        "          \"\"\"\n",
        "          Detect and replace outliers using IQR and linear interpolation.\n",
        "\n",
        "          Parameters:\n",
        "          - df: pd.DataFrame — Input DataFrame.\n",
        "          - columns: list — Columns to check for outliers.\n",
        "          - factor: float — IQR multiplier for determining outlier thresholds (default=1.5).\n",
        "\n",
        "          Returns:\n",
        "          - pd.DataFrame — DataFrame with outliers replaced via interpolation.\n",
        "          \"\"\"\n",
        "          df_clean = df.copy()\n",
        "\n",
        "          for col in columns:\n",
        "              if col in [\"Date\", \"Time\"]:\n",
        "                  continue\n",
        "\n",
        "              Q1 = df_clean[col].quantile(0.25)\n",
        "              Q3 = df_clean[col].quantile(0.75)\n",
        "              IQR = Q3 - Q1\n",
        "              lower_bound = Q1 - factor * IQR\n",
        "              upper_bound = Q3 + factor * IQR\n",
        "\n",
        "              # Mark outliers as NaN\n",
        "              mask_outliers = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
        "              df_clean.loc[mask_outliers, col] = np.nan\n",
        "\n",
        "              # Interpolate to fill in outliers\n",
        "              df_clean[col] = df_clean[col].interpolate(limit_direction='both')\n",
        "\n",
        "          return df_clean\n",
        "\n",
        "\n",
        "        cols1 = [col for col in one.columns if col not in [\"Date\", \"Time\"]]\n",
        "        cols2 = [col for col in two.columns if col not in [\"Date\", \"Time\"]]\n",
        "        cols3 = [col for col in three.columns if col not in [\"Date\", \"Time\"]]\n",
        "\n",
        "        one = interpolate_outliers(one, cols1)\n",
        "        two = interpolate_outliers(two, cols2)\n",
        "        three = interpolate_outliers(three, cols3)\n",
        "\n",
        "        one = fill_missing_solar_data(one, cols1)\n",
        "        two = fill_missing_solar_data(two, cols2)\n",
        "        three = fill_missing_solar_data(three, cols3)\n",
        "\n",
        "        return one, two, three\n",
        "\n",
        "    def combine_inverter_dfs(self, one, two, three):\n",
        "        \"\"\"\n",
        "        Combine the three inverter DataFrames into a single DataFrame.\n",
        "        \"\"\"\n",
        "        # Rename columns to include inverter number suffixes\n",
        "        one = one.rename(columns={col: f\"{col}_inv1\" for col in one.columns if col not in [\"Date\", \"Time\"]})\n",
        "        two = two.rename(columns={col: f\"{col}_inv2\" for col in two.columns if col not in [\"Date\", \"Time\"]})\n",
        "        three = three.rename(columns={col: f\"{col}_inv3\" for col in three.columns if col not in [\"Date\", \"Time\"]})\n",
        "\n",
        "        # Merge the three DataFrames on \"Date\" and \"Time\"\n",
        "        df = one.merge(two, on=[\"Date\", \"Time\"]).merge(three, on=[\"Date\", \"Time\"])\n",
        "\n",
        "        # Identify unique metric types by extracting column names without inverter numbers\n",
        "        metric_types = set(col.rsplit(\"_\", 1)[0] for col in df.columns if col not in [\"Date\", \"Time\"])\n",
        "\n",
        "        # Aggregate metrics\n",
        "        for metric in metric_types:\n",
        "            # Select all inverter columns for this metric based on suffixes\n",
        "            columns_to_combine = [col for col in df.columns if col.startswith(metric) and col not in [\"Date\", \"Time\"]]\n",
        "\n",
        "            # Combine the columns (sum or mean, depending on metric type)\n",
        "            if any(word in metric.lower() for word in [\"power\", \"energy\", \"connections\"]):\n",
        "                df[f\"total_{metric}\"] = df[columns_to_combine].sum(axis=1)\n",
        "            elif any(word in metric.lower() for word in [\"temperature\", \"average\", \"percent\"]):\n",
        "                df[f\"average_{metric}\"] = df[columns_to_combine].mean(axis=1)\n",
        "            else:\n",
        "                df[f\"total_{metric}\"] = df[columns_to_combine].sum(axis=1)\n",
        "\n",
        "            # Drop the original columns for this metric\n",
        "            df.drop(columns=columns_to_combine, inplace=True)\n",
        "\n",
        "        self.combined_df = df\n",
        "        return self\n",
        "\n",
        "\n",
        "    def add_datetime_and_hour(self):\n",
        "        self.combined_df['Datetime'] = pd.to_datetime(self.combined_df['Date'] + ' ' + self.combined_df['Time'])\n",
        "        self.combined_df['hour'] = self.combined_df['Datetime'].dt.hour\n",
        "\n",
        "        self.weather_df['Datetime'] = pd.to_datetime(self.weather_df['Date'] + ' ' + self.weather_df['Time'])\n",
        "        self.weather_df['hour'] = self.weather_df['Datetime'].dt.hour\n",
        "\n",
        "        self.combined_df = self.combined_df.sort_values('Datetime')\n",
        "        self.weather_df = self.weather_df.sort_values('Datetime')\n",
        "\n",
        "        return self\n",
        "\n",
        "    def merge_weather_data(self):\n",
        "        self.combined_df = pd.merge_asof(self.combined_df, self.weather_df, on='Datetime', direction='nearest')\n",
        "\n",
        "        self.combined_df = self.combined_df.drop(columns=['hour_x', 'Date_y', 'Time_y'])\n",
        "        self.combined_df = self.combined_df.rename(columns={\n",
        "            'Date_x':'date',\n",
        "            'Time_x':'time',\n",
        "            'hour_y':'hour'\n",
        "        })\n",
        "        return self\n",
        "\n",
        "    def resample_time_series(self, data, interval='10min', tolerance='5min'):\n",
        "        \"\"\"\n",
        "        Resample a time series to a consistent interval using closest real values when available,\n",
        "        otherwise interpolate.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : pandas.DataFrame\n",
        "            Original time series data with a 'Datetime' column or 'date' and 'time' columns.\n",
        "        interval : str\n",
        "            Desired resampling interval (e.g., '10min').\n",
        "        tolerance : str\n",
        "            Time window to search for a nearby real value (e.g., '2min').\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pandas.DataFrame\n",
        "            Resampled DataFrame with consistent intervals using closest available data or interpolation.\n",
        "        \"\"\"\n",
        "\n",
        "        df = data.copy()\n",
        "\n",
        "        if 'Datetime' in df.columns:\n",
        "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "        else:\n",
        "            df['Datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "\n",
        "        df.set_index('Datetime', inplace=True)\n",
        "        df.sort_index(inplace=True)\n",
        "\n",
        "        original_index = df.index\n",
        "        resample_index = pd.date_range(start=original_index.min(), end=original_index.max(), freq=interval)\n",
        "        tolerance_offset = pd.to_timedelta(tolerance)\n",
        "\n",
        "        # Initialize resampled DataFrame\n",
        "        df_resampled = pd.DataFrame(index=resample_index)\n",
        "\n",
        "        numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            resampled_col = []\n",
        "\n",
        "            for target_time in resample_index:\n",
        "                # Find the closest existing time within tolerance\n",
        "                mask = (original_index >= target_time - tolerance_offset) & (original_index <= target_time + tolerance_offset)\n",
        "                candidates = df.loc[mask, col]\n",
        "\n",
        "                if not candidates.empty:\n",
        "                    # Use the value closest in time\n",
        "                    closest_time = candidates.index[np.argmin(np.abs(candidates.index - target_time))]\n",
        "                    resampled_col.append(df.loc[closest_time, col])\n",
        "                else:\n",
        "                    resampled_col.append(np.nan)\n",
        "\n",
        "            df_resampled[col] = resampled_col\n",
        "\n",
        "            # Interpolate any remaining missing values\n",
        "            df_resampled[col] = df_resampled[col].interpolate(method='time', limit_direction='both')\n",
        "\n",
        "        # Final fill if needed (still NaNs after interpolation)\n",
        "        df_resampled = df_resampled.ffill().bfill()\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\nResampled to {interval} with tolerance {tolerance}\")\n",
        "        print(f\"Resulting rows: {len(df_resampled)}\")\n",
        "        remaining_nans = df_resampled.isna().sum()\n",
        "        if remaining_nans.sum() > 0:\n",
        "            print(\"Warning: Columns with remaining NaNs:\")\n",
        "            print(remaining_nans[remaining_nans > 0])\n",
        "        else:\n",
        "            print(\"All missing values successfully filled.\")\n",
        "\n",
        "        self.combined_df = df_resampled\n",
        "        return self\n",
        "\n",
        "\n",
        "    def process_data(self, inverter_csv_path, weather_csv_path):\n",
        "        \"\"\"\n",
        "        Process the data from start to finish in a single method call.\n",
        "        ****** If you want to use the resample_time_series method, you will have to add to this method. I added it so we can get the timestamps at the same resolution\n",
        "            it automatically updates the combined_df so all you should have to add to this method is self.resample_time_series()\n",
        "\n",
        "        \"\"\"\n",
        "        self.read_data(inverter_csv_path, weather_csv_path)\n",
        "        one, two, three = self.split_inverter_data()\n",
        "        one, two, three = self.fill_missing_data(one, two, three)\n",
        "        self.combine_inverter_dfs(one, two, three)\n",
        "        self.add_datetime_and_hour()\n",
        "        self.merge_weather_data()\n",
        "        self.resample_time_series(data=self.combined_df)\n",
        "\n",
        "        return self.combined_df  # Ensure you return the actual processed DataFrame\n",
        "\n",
        "# example usage\n",
        "processor = SolarDataProcessor()\n",
        "df = processor.process_data('Inv_2024-09-30.csv', 'weather_2024-09-30.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "eMSFlh5iidTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "class LSTMForecastModel:\n",
        "    def __init__(self, data=None, file_path=None, lookback=5): # lookback is how many rows back in the data the model is seeing. you will need to adjust this if you\n",
        "                                                              # adjsut how you resample the data (I commented on this below in the preprocess data method)\n",
        "        \"\"\"\n",
        "        Initialize the LSTM model with either a DataFrame or a CSV file path.\n",
        "        Parameters:\n",
        "        - data: pandas DataFrame containing the input data.\n",
        "        - file_path: path to the CSV file containing the data.\n",
        "        - lookback: Number of past timesteps to use as input for LSTM.\n",
        "        \"\"\"\n",
        "        if data is not None:\n",
        "            self.df = data.copy()\n",
        "        elif file_path is not None:\n",
        "            self.df = pd.read_csv(file_path)\n",
        "        else:\n",
        "            raise ValueError(\"Either a DataFrame or a file path must be provided.\")\n",
        "        self.lookback = lookback\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.features = [\n",
        "            'historical_power','average_temperature_C', 'WMS 01 irradiance', 'hours_since_sunrise','hours_until_sunset','peak_sun_factor','is_daytime',\n",
        "            'total_last_active_fault', 'total_net_AC_voltage_V','total_time_run_today_h', 'average_average_cosphii_percent',\n",
        "       'total_DC_voltage_DCV', 'total_output_current_A',\n",
        "       'total_grid_connections','total_net_frequency_Hz'\n",
        "        ]# is_daytime\n",
        "\n",
        "        # Define what features to engineer and how\n",
        "        self.feature_engineering_plan = {\n",
        "            'total_output_power_kW': {\n",
        "                'lags': [1, 2, 3, 6, 12, 24],\n",
        "                'rolling': {'mean': [1,3, 6, 12,24], 'std': [3,6], 'min': [3,6], 'max': [1,2,3,6]},\n",
        "                'diff': [1, 2,3,4,5]\n",
        "            },\n",
        "            'average_temperature_C': {\n",
        "                'lags': [1, 3, 6, 12],\n",
        "                'rolling': {'mean': [1,2,3, 6,12,24], 'std': [2,3,6]},\n",
        "                'diff': [1, 2,3]\n",
        "            },\n",
        "            'total_DC_voltage_DCV': {\n",
        "                'lags': [1, 3, 6],\n",
        "                'rolling': {'mean': [1,2,3, 6,12,24], 'std': [2,3,6]},\n",
        "                'diff': [1, 2,3]\n",
        "            },\n",
        "            'total_output_current_A': {\n",
        "                'lags': [1, 3, 6],\n",
        "                'rolling': {'mean': [1,2,3, 6,12,24], 'std': [2,3,6]},\n",
        "                'diff': [1, 2,3]\n",
        "            },\n",
        "            'average_average_cosphii_percent': {\n",
        "                'lags': [1, 3, 6],\n",
        "                'rolling': {'mean': [1,2,3, 6,12,24]},\n",
        "                'diff': [1, 2,3]\n",
        "            },\n",
        "            'total_time_run_today_h': {\n",
        "                'lags': [1, 3, 6],\n",
        "                'rolling': {'sum': [1,3, 6,12,24]},\n",
        "                'diff': [1, 2,3]\n",
        "            },\n",
        "            'total_last_active_fault': {\n",
        "                'lags': [1, 3],\n",
        "                'rolling': {'sum': [1,3, 6,12,24]},\n",
        "                'diff': [1, 2,3]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if 'WMS 01 irradiance' in self.df.columns:\n",
        "            self.feature_engineering_plan['WMS 01 irradiance'] = {\n",
        "                'lags': [1, 2, 3, 6, 12, 24],\n",
        "                'rolling': {'mean': [1,3, 6, 12], 'std': [2,3,6], 'max': [1,2, 3,6]},\n",
        "                'diff': [1, 2, 3,4,5]\n",
        "            }\n",
        "        self.target = None\n",
        "\n",
        "        '''\n",
        "       for self.features you can add more columns to this if you want. but these are some of the ones that i have been using.\n",
        "        '''\n",
        "\n",
        "    def preprocess_data(self, resample_freq='30min', target_horizon=18):\n",
        "        \"\"\"\n",
        "        Preprocess the dataset for LSTM input using XGBoost-style resampling.\n",
        "        - resample_freq: frequency string (e.g., '10min', '30min')\n",
        "        - target_horizon: number of rows (at original freq) to shift target forward\n",
        "        \"\"\"\n",
        "        if 'datetime' in self.df.columns:\n",
        "            self.df['datetime'] = pd.to_datetime(self.df['datetime'])\n",
        "            self.df.set_index('datetime', inplace=True)\n",
        "        elif 'index' in self.df.columns:\n",
        "            self.df['datetime'] = pd.to_datetime(self.df['index'])\n",
        "            self.df.set_index('datetime', inplace=True)\n",
        "\n",
        "        self.df['hour'] = self.df.index.hour\n",
        "        self.df['day'] = self.df.index.day\n",
        "        self.df['month'] = self.df.index.month\n",
        "        self.df['dayofweek'] = self.df.index.dayofweek\n",
        "        self.df['quarter'] = self.df.index.quarter\n",
        "        self.df['minute'] = self.df.index.minute\n",
        "\n",
        "        self.df['hours_since_sunrise'] = (self.df['hour'] + self.df['minute'] / 60 - 6).clip(lower=0)\n",
        "        self.df['hours_until_sunset'] = (18 - (self.df['hour'] + self.df['minute'] / 60)).clip(lower=0)\n",
        "        self.df['is_daytime'] = ((self.df['hour'] >= 6) & (self.df['hour'] < 18)).astype(int)\n",
        "\n",
        "        self.df['peak_sun_factor'] = np.sin(np.pi * (self.df['hour'] + self.df['minute']/60 - 6) / 12)\n",
        "        self.df['peak_sun_factor'] = self.df['peak_sun_factor'].clip(lower=0)\n",
        "\n",
        "        self.df['hour_sin'] = np.sin(2 * np.pi * self.df['hour'] / 24)\n",
        "        self.df['hour_cos'] = np.cos(2 * np.pi * self.df['hour'] / 24)\n",
        "        self.df['month_sin'] = np.sin(2 * np.pi * self.df['month'] / 12)\n",
        "        self.df['month_cos'] = np.cos(2 * np.pi * self.df['month'] / 12)\n",
        "\n",
        "        # Original target\n",
        "        original_power = self.df['total_output_power_kW'].copy()\n",
        "\n",
        "        # Resample non-target features\n",
        "        df_features = self.df.drop(columns=['total_output_power_kW'])\n",
        "        df_resampled = df_features.select_dtypes(include=[np.number]).resample(resample_freq).mean()\n",
        "\n",
        "        # Resample power for input (to include historical context)\n",
        "        df_resampled['historical_power'] = original_power.resample(resample_freq).mean()\n",
        "\n",
        "        # Shift original target to create a future prediction target\n",
        "        future_target = original_power.shift(-target_horizon)\n",
        "        df_resampled['power_future'] = future_target.resample(resample_freq).first()\n",
        "\n",
        "        # Align future target to df_resampled\n",
        "        common_idx = df_resampled.index.intersection(future_target.index)\n",
        "        df_resampled = df_resampled.loc[common_idx].copy()\n",
        "        df_resampled['power_future'] = future_target.loc[common_idx]\n",
        "\n",
        "        # Feature engineering\n",
        "        for col, ops in self.feature_engineering_plan.items():\n",
        "            if col not in df_resampled.columns:\n",
        "                print(f\"Warning: {col} not found in dataframe, skipping\")\n",
        "                continue\n",
        "\n",
        "            if 'lags' in ops:\n",
        "                for lag in ops['lags']:\n",
        "                    feature_name = f'{col}_lag_{lag}'\n",
        "                    df_resampled[feature_name] = df_resampled[col].shift(lag)\n",
        "                    self.features.append(feature_name)\n",
        "\n",
        "            if 'rolling' in ops:\n",
        "                for method, windows in ops['rolling'].items():\n",
        "                    for window in windows:\n",
        "                        feature_name = f'{col}_{method}_{window}'\n",
        "                        if method == 'mean':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).mean()\n",
        "                        elif method == 'sum':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).sum()\n",
        "                        elif method == 'std':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).std()\n",
        "                        elif method == 'min':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).min()\n",
        "                        elif method == 'max':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).max()\n",
        "                        self.features.append(feature_name)\n",
        "\n",
        "            if 'diff' in ops:\n",
        "                for periods in ops['diff']:\n",
        "                    feature_name = f'{col}_diff_{periods}'\n",
        "                    df_resampled[feature_name] = df_resampled[col].diff(periods=periods)\n",
        "                    self.features.append(feature_name)\n",
        "\n",
        "        self.df_clean = df_resampled.dropna()\n",
        "\n",
        "        # Scale input features only\n",
        "        self.df_clean[self.features] = self.scaler.fit_transform(self.df_clean[self.features])\n",
        "\n",
        "        # add for later use in blender\n",
        "        self.target = self.df_clean['power_future']\n",
        "\n",
        "    def create_sequences(self):\n",
        "        \"\"\"Convert the dataframe into sequences for LSTM input.\"\"\"\n",
        "        X, y = [], []\n",
        "        data = self.df_clean[self.features].values\n",
        "        target = self.df_clean['power_future'].values\n",
        "        for i in range(len(data) - self.lookback):\n",
        "            X.append(data[i:i + self.lookback])\n",
        "            y.append(target[i + self.lookback])\n",
        "        self.X = np.array(X)\n",
        "        self.y = np.array(y)\n",
        "\n",
        "        # Train-test split (80% train, 20% test)\n",
        "        split_idx = int(len(self.X) * 0.8)\n",
        "        self.X_train, self.X_test = self.X[:split_idx], self.X[split_idx:]\n",
        "        self.y_train, self.y_test = self.y[:split_idx], self.y[split_idx:]\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Define the LSTM model architecture.\"\"\"\n",
        "        model = Sequential([\n",
        "            LSTM(128, activation='tanh', return_sequences=True, input_shape=(self.lookback, len(self.features))),\n",
        "            Dropout(0.2),\n",
        "            LSTM(64, activation='tanh'),\n",
        "            Dropout(0.2),\n",
        "            Dense(50, activation='relu'),\n",
        "            Dense(25, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "        self.model = model\n",
        "\n",
        "    def train_model(self, epochs=50, batch_size=16):\n",
        "        \"\"\"Train the LSTM model and store history for plotting.\"\"\"\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            self.X_train, self.y_train, epochs=epochs, batch_size=batch_size,\n",
        "            validation_data=(self.X_test, self.y_test), verbose=1, callbacks=[early_stopping]\n",
        "        )\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"Evaluate the LSTM model on the test set.\"\"\"\n",
        "        y_pred = self.model.predict(self.X_test)\n",
        "        y_pred = np.clip(y_pred, a_min=0, a_max=None)  # for non-negative predictions\n",
        "        self.metrics = {\n",
        "            'rmse': np.sqrt(mean_squared_error(self.y_test, y_pred)),\n",
        "            'mae': mean_absolute_error(self.y_test, y_pred),\n",
        "            'r2': r2_score(self.y_test, y_pred)\n",
        "        }\n",
        "        return self.metrics\n",
        "\n",
        "    def plot_results(self):\n",
        "        \"\"\"Plot actual vs predicted power output.\"\"\"\n",
        "        y_pred = self.model.predict(self.X_test)\n",
        "        y_pred = np.clip(y_pred, a_min=0, a_max=None)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(self.y_test, label='Actual', alpha=0.8)\n",
        "        plt.plot(y_pred, label='Predicted', alpha=0.8)\n",
        "        plt.title('Hourly Solar Power Prediction (LSTM, 3 hours ahead)')\n",
        "        plt.ylabel('Power Output (kW)')\n",
        "        plt.xlabel('Time')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training & validation loss and MAE.\"\"\"\n",
        "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "        # Plot loss\n",
        "        ax1.plot(self.history.history['loss'], label='Train Loss', color='blue')\n",
        "        ax1.plot(self.history.history['val_loss'], label='Validation Loss', color='blue', linestyle='dashed')\n",
        "        ax1.set_xlabel('Epochs')\n",
        "        ax1.set_ylabel('Loss (MSE)')\n",
        "        ax1.set_title('Training & Validation Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "        # Plot MAE\n",
        "        fig, ax2 = plt.subplots(figsize=(12, 6))\n",
        "        ax2.plot(self.history.history['mae'], label='Train MAE', color='red')\n",
        "        ax2.plot(self.history.history['val_mae'], label='Validation MAE', color='red', linestyle='dashed')\n",
        "        ax2.set_xlabel('Epochs')\n",
        "        ax2.set_ylabel('MAE')\n",
        "        ax2.set_title('Training & Validation MAE')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    def predict(self, new_data, resample_freq='30min', target_horizon=18):\n",
        "        \"\"\"\n",
        "        Generate predictions on new, preprocessed data.\n",
        "\n",
        "        Parameters:\n",
        "        - new_data: pandas DataFrame with original structure (before resampling).\n",
        "        - resample_freq: resampling frequency to match training.\n",
        "        - target_horizon: number of future steps predicted ahead.\n",
        "\n",
        "        Returns:\n",
        "        - predictions: pandas Series with timestamps aligned.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built or trained yet. Call train_model() first.\")\n",
        "        if not hasattr(self, 'features') or not self.features:\n",
        "            raise ValueError(\"Feature list not found. Ensure preprocess_data() was called before training.\")\n",
        "\n",
        "        # Store original features to avoid modifying them\n",
        "        original_features = self.features.copy()\n",
        "\n",
        "        # Clone and preprocess new data\n",
        "        temp_df = new_data.copy()\n",
        "\n",
        "        if 'datetime' in temp_df.columns:\n",
        "            temp_df['datetime'] = pd.to_datetime(temp_df['datetime'])\n",
        "            temp_df.set_index('datetime', inplace=True)\n",
        "        elif 'index' in temp_df.columns:\n",
        "            temp_df['datetime'] = pd.to_datetime(temp_df['index'])\n",
        "            temp_df.set_index('datetime', inplace=True)\n",
        "\n",
        "        temp_df['hour'] = temp_df.index.hour\n",
        "        temp_df['day'] = temp_df.index.day\n",
        "        temp_df['month'] = temp_df.index.month\n",
        "        temp_df['dayofweek'] = temp_df.index.dayofweek\n",
        "        temp_df['quarter'] = temp_df.index.quarter\n",
        "        temp_df['minute'] = temp_df.index.minute\n",
        "\n",
        "        temp_df['hours_since_sunrise'] = (temp_df['hour'] + temp_df['minute'] / 60 - 6).clip(lower=0)\n",
        "        temp_df['hours_until_sunset'] = (18 - (temp_df['hour'] + temp_df['minute'] / 60)).clip(lower=0)\n",
        "        temp_df['is_daytime'] = ((temp_df['hour'] >= 6) & (temp_df['hour'] < 18)).astype(int)\n",
        "\n",
        "        temp_df['peak_sun_factor'] = np.sin(np.pi * (temp_df['hour'] + temp_df['minute']/60 - 6) / 12)\n",
        "        temp_df['peak_sun_factor'] = temp_df['peak_sun_factor'].clip(lower=0)\n",
        "\n",
        "        temp_df['hour_sin'] = np.sin(2 * np.pi * temp_df['hour'] / 24)\n",
        "        temp_df['hour_cos'] = np.cos(2 * np.pi * temp_df['hour'] / 24)\n",
        "        temp_df['month_sin'] = np.sin(2 * np.pi * temp_df['month'] / 12)\n",
        "        temp_df['month_cos'] = np.cos(2 * np.pi * temp_df['month'] / 12)\n",
        "\n",
        "        original_power = temp_df['total_output_power_kW'].copy()\n",
        "\n",
        "        df_features = temp_df.drop(columns=['total_output_power_kW'])\n",
        "        df_resampled = df_features.select_dtypes(include=[np.number]).resample(resample_freq).mean()\n",
        "\n",
        "        df_resampled['historical_power'] = original_power.resample(resample_freq).mean()\n",
        "\n",
        "        future_target = original_power.shift(-target_horizon)\n",
        "        df_resampled['power_future'] = future_target.resample(resample_freq).first()\n",
        "\n",
        "        common_idx = df_resampled.index.intersection(future_target.index)\n",
        "        df_resampled = df_resampled.loc[common_idx].copy()\n",
        "        df_resampled['power_future'] = future_target.loc[common_idx]\n",
        "\n",
        "        for col, ops in self.feature_engineering_plan.items():\n",
        "            if col not in df_resampled.columns:\n",
        "                print(f\"Warning: {col} not found in dataframe, skipping\")\n",
        "                continue\n",
        "\n",
        "            if 'lags' in ops:\n",
        "                for lag in ops['lags']:\n",
        "                    feature_name = f'{col}_lag_{lag}'\n",
        "                    df_resampled[feature_name] = df_resampled[col].shift(lag)\n",
        "\n",
        "            if 'rolling' in ops:\n",
        "                for method, windows in ops['rolling'].items():\n",
        "                    for window in windows:\n",
        "                        feature_name = f'{col}_{method}_{window}'\n",
        "                        if method == 'mean':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).mean()\n",
        "                        elif method == 'sum':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).sum()\n",
        "                        elif method == 'std':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).std()\n",
        "                        elif method == 'min':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).min()\n",
        "                        elif method == 'max':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).max()\n",
        "\n",
        "            if 'diff' in ops:\n",
        "                for periods in ops['diff']:\n",
        "                    feature_name = f'{col}_diff_{periods}'\n",
        "                    df_resampled[feature_name] = df_resampled[col].diff(periods=periods)\n",
        "\n",
        "        df_resampled.dropna(inplace=True)\n",
        "\n",
        "        missing_features = [f for f in original_features if f not in df_resampled.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing features in prediction data: {missing_features}\")\n",
        "\n",
        "        df_resampled[original_features] = self.scaler.transform(df_resampled[original_features])\n",
        "\n",
        "        # Create sequences\n",
        "        X_pred = []\n",
        "        index = []\n",
        "        data = df_resampled[original_features].values\n",
        "        for i in range(len(data) - self.lookback - target_horizon + 1):\n",
        "            X_pred.append(data[i:i + self.lookback])\n",
        "            index.append(df_resampled.index[i + self.lookback + target_horizon - 1])  # Align prediction with future\n",
        "\n",
        "        X_pred = np.array(X_pred)\n",
        "\n",
        "        if len(X_pred) == 0:\n",
        "            raise ValueError(\"Not enough data to generate sequences for prediction.\")\n",
        "\n",
        "        predictions = self.model.predict(X_pred)\n",
        "        predictions = np.clip(predictions, a_min=0, a_max=None)\n",
        "\n",
        "        return pd.Series(predictions.flatten(), index=index, name='lstm_predictions')\n",
        "\n",
        "\n",
        "    def run_pipeline(self, epochs=100, batch_size=64):\n",
        "        \"\"\"Run the full pipeline: preprocessing, sequence creation, training, evaluating, and plotting.\"\"\"\n",
        "        self.preprocess_data()\n",
        "        self.create_sequences()\n",
        "        self.build_model()\n",
        "        self.train_model(epochs=epochs, batch_size=batch_size)\n",
        "        results = self.evaluate_model()\n",
        "        self.plot_results()\n",
        "        self.plot_training_history()\n",
        "        return results\n",
        "\n",
        "# model = LSTMForecastModel(data=df)\n",
        "# results = model.run_pipeline()\n",
        "# results"
      ],
      "metadata": {
        "id": "2K-q_GzGiaPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "F4kXnJiLimeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "import seaborn as sns\n",
        "\n",
        "class ImprovedXGBoostModel:\n",
        "    def __init__(self, data=None, file_path=None, direct_features=None):\n",
        "        if data is not None:\n",
        "            self.df = data.copy()\n",
        "        elif file_path is not None:\n",
        "            self.df = pd.read_csv(file_path)\n",
        "        else:\n",
        "            raise ValueError(\"Provide either a DataFrame or file path\")\n",
        "\n",
        "        # Allow user to specify which features to use directly\n",
        "        #self.direct_features = direct_features or ['WMS 01 irradiancee']\n",
        "\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.model = None\n",
        "\n",
        "        self.target = None\n",
        "\n",
        "        # Starting with basic features\n",
        "        self.features = []\n",
        "\n",
        "        # Direct features to use (these are passed directly to the model without transformations)\n",
        "        self.direct_features = ['WMS 01 irradiance','average_temperature_C', 'total_output_power_kW',\n",
        "            'total_last_active_fault', 'total_net_AC_voltage_V','total_time_run_today_h', 'average_average_cosphii_percent',\n",
        "       'total_DC_voltage_DCV', 'total_output_current_A',\n",
        "       'total_grid_connections','total_net_frequency_Hz']\n",
        "\n",
        "        # Define what features to engineer and how\n",
        "        self.feature_engineering_plan = {\n",
        "            'total_output_power_kW': {\n",
        "                'lags': [1, 2, 3, 6, 12, 24],  # Expanded lag windows\n",
        "                'rolling': {'mean': [1,3, 6, 12,24], 'std': [3,6], 'min': [3,6], 'max': [1,2,3,6]},\n",
        "                'diff': [1, 2,3,4,5]  # Rate of change in power\n",
        "            },\n",
        "            'average_temperature_C': {\n",
        "                'lags': [1, 3, 6, 12],\n",
        "                'rolling': {'mean': [1,2,3, 6,12,24], 'std': [2,3,6]},\n",
        "                'diff': [1, 2,3]  # Temperature change rate\n",
        "            },\n",
        "            'total_DC_voltage_DCV': {\n",
        "                'lags': [1, 3, 6],\n",
        "                'rolling': {'mean': [1,2,3, 6,12,24], 'std': [2,3,6]},\n",
        "                'diff': [1, 2,3]  # Voltage change rate\n",
        "            },\n",
        "            'total_output_current_A': {\n",
        "                'lags': [1, 3, 6],\n",
        "                'rolling': {'mean': [1,2,3, 6,12,24], 'std': [2,3,6]},\n",
        "                'diff': [1, 2,3]\n",
        "            },\n",
        "            'average_average_cosphii_percent': {\n",
        "                'lags': [1, 3, 6],\n",
        "                'rolling': {'mean': [1,2,3, 6,12,24]},\n",
        "                'diff': [1, 2,3]\n",
        "            },\n",
        "            'total_time_run_today_h': {\n",
        "                'lags': [1, 3, 6],\n",
        "                'rolling': {'sum': [1,3, 6,12,24]},\n",
        "                'diff': [1, 2,3]\n",
        "            },\n",
        "            'total_last_active_fault': {\n",
        "                'lags': [1, 3],\n",
        "                'rolling': {'sum': [1,3, 6,12,24]},\n",
        "                'diff': [1, 2,3]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # If solar irradiance is available, add it to plan\n",
        "        if 'WMS 01 irradiance' in self.df.columns:\n",
        "            self.feature_engineering_plan['WMS 01 irradiance'] = {\n",
        "                'lags': [0, 1, 2, 3, 6, 12, 24],\n",
        "                'rolling': {'mean': [1,3, 6, 12], 'std': [2,3,6], 'max': [1,2, 3,6]},\n",
        "                'diff': [1, 2, 3,4,5]  # Rate of change in irradiance\n",
        "            }\n",
        "\n",
        "    def preprocess_data(self, target_horizon=18, resample_freq='10T'):\n",
        "        \"\"\"\n",
        "        Preprocess data with more sophisticated feature engineering\n",
        "\n",
        "        Parameters:\n",
        "        - target_horizon: Number of periods ahead to predict\n",
        "        - resample_freq: Frequency to resample data ('60T' for hourly)\n",
        "        \"\"\"\n",
        "        # Parse datetime\n",
        "        if 'datetime' in self.df.columns:\n",
        "            self.df['datetime'] = pd.to_datetime(self.df['datetime'])\n",
        "            self.df.set_index('datetime', inplace=True)\n",
        "        elif 'index' in self.df.columns:\n",
        "            self.df['datetime'] = pd.to_datetime(self.df['index'])\n",
        "            self.df.set_index('datetime', inplace=True)\n",
        "        else:\n",
        "          # If no datetime column is found, try to infer from 'date' and 'time'\n",
        "          try:\n",
        "              self.df['datetime'] = pd.to_datetime(self.df['date'] + ' ' + self.df['time'])\n",
        "              self.df.set_index('datetime', inplace=True)\n",
        "          except KeyError:\n",
        "              raise ValueError(\"No valid datetime information found. Please provide a 'datetime', 'date' and 'time', or 'index' column.\")\n",
        "\n",
        "        # Enhanced time-based features\n",
        "        self.df['hour'] = self.df.index.hour\n",
        "        self.df['day'] = self.df.index.day\n",
        "        self.df['month'] = self.df.index.month\n",
        "        self.df['dayofweek'] = self.df.index.dayofweek\n",
        "        self.df['quarter'] = self.df.index.quarter\n",
        "        self.df['minute'] = self.df.index.minute\n",
        "\n",
        "        # Solar position-related features (improved)\n",
        "        self.df['hours_since_sunrise'] = (self.df['hour'] + self.df['minute'] / 60 - 6).clip(lower=0)\n",
        "        self.df['hours_until_sunset'] = (18 - (self.df['hour'] + self.df['minute'] / 60)).clip(lower=0)\n",
        "        self.df['is_daytime'] = ((self.df['hour'] >= 6) & (self.df['hour'] < 18)).astype(int)\n",
        "\n",
        "        # Peak sun hours approximation (simplified model)\n",
        "        self.df['peak_sun_factor'] = np.sin(np.pi * (self.df['hour'] + self.df['minute']/60 - 6) / 12)\n",
        "        self.df['peak_sun_factor'] = self.df['peak_sun_factor'].clip(lower=0)\n",
        "\n",
        "        # Create cyclic time features to better capture periodicity\n",
        "        self.df['hour_sin'] = np.sin(2 * np.pi * self.df['hour'] / 24)\n",
        "        self.df['hour_cos'] = np.cos(2 * np.pi * self.df['hour'] / 24)\n",
        "        self.df['month_sin'] = np.sin(2 * np.pi * self.df['month'] / 12)\n",
        "        self.df['month_cos'] = np.cos(2 * np.pi * self.df['month'] / 12)\n",
        "\n",
        "        # Step 0: Shift irradiance to simulate known-ahead forecast\n",
        "        self.df['irradiance_future'] = self.df['WMS 01 irradiance'].shift(-target_horizon)\n",
        "\n",
        "        # Step 1: Keep the original high-frequency target series\n",
        "        original_target = self.df['total_output_power_kW'].copy()\n",
        "        future_target = original_target.shift(-target_horizon)\n",
        "        irradiance_future = self.df['irradiance_future']\n",
        "\n",
        "        # Step 2: Resample all numeric feature columns except the target\n",
        "        df_features = self.df.drop(columns=['total_output_power_kW'])\n",
        "        df_resampled = df_features.select_dtypes(include=[np.number]).resample(resample_freq).mean()\n",
        "\n",
        "        # Step 3: Resample future target and future irradiance to match resample frequency\n",
        "        df_resampled['power_future'] = future_target.resample(resample_freq).first()\n",
        "        df_resampled['irradiance_future'] = irradiance_future.resample(resample_freq).first()\n",
        "\n",
        "        # Step 4: Align everything using a common index\n",
        "        common_idx = df_resampled.index.intersection(df_resampled['power_future'].dropna().index)\n",
        "        df_resampled = df_resampled.loc[common_idx].copy()\n",
        "        df_resampled['power_future'] = df_resampled['power_future'].loc[common_idx]\n",
        "        df_resampled['irradiance_future'] = df_resampled['irradiance_future'].loc[common_idx]\n",
        "\n",
        "        # Feature engineering based on plan\n",
        "        for col, ops in self.feature_engineering_plan.items():\n",
        "            if col not in df_resampled.columns:\n",
        "                print(f\"Warning: {col} not found in dataframe, skipping\")\n",
        "                continue\n",
        "\n",
        "            # Create lag features\n",
        "            if 'lags' in ops:\n",
        "                for lag in ops['lags']:\n",
        "                    feature_name = f'{col}_lag_{lag}'\n",
        "                    df_resampled[feature_name] = df_resampled[col].shift(lag)\n",
        "                    self.features.append(feature_name)\n",
        "\n",
        "            # Create rolling window features\n",
        "            if 'rolling' in ops:\n",
        "                for method, windows in ops['rolling'].items():\n",
        "                    for window in windows:\n",
        "                        feature_name = f'{col}_{method}_{window}'\n",
        "                        if method == 'mean':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).mean()\n",
        "                        elif method == 'sum':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).sum()\n",
        "                        elif method == 'std':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).std()\n",
        "                        elif method == 'min':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).min()\n",
        "                        elif method == 'max':\n",
        "                            df_resampled[feature_name] = df_resampled[col].rolling(window=window).max()\n",
        "                        self.features.append(feature_name)\n",
        "\n",
        "            # Create difference features (rate of change)\n",
        "            if 'diff' in ops:\n",
        "                for periods in ops['diff']:\n",
        "                    feature_name = f'{col}_diff_{periods}'\n",
        "                    df_resampled[feature_name] = df_resampled[col].diff(periods=periods)\n",
        "                    self.features.append(feature_name)\n",
        "\n",
        "        # Add time features to feature list\n",
        "        time_features = ['hour', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
        "                         'hours_since_sunrise', 'hours_until_sunset', 'is_daytime',\n",
        "                         'peak_sun_factor']\n",
        "\n",
        "        df_resampled[time_features] = self.df[time_features]\n",
        "        self.features += time_features\n",
        "\n",
        "        # Create interaction features if solar irradiance is available\n",
        "        if 'WMS 01 irradiance' in df_resampled.columns:\n",
        "            df_resampled['irr_temp_interaction'] = df_resampled['WMS 01 irradiance'] * df_resampled['average_temperature_C']\n",
        "            df_resampled['irr_time_interaction'] = df_resampled['WMS 01 irradiance'] * df_resampled['peak_sun_factor']\n",
        "            self.features += ['irr_temp_interaction', 'irr_time_interaction']\n",
        "\n",
        "        # Create target (future power output)\n",
        "        #df_resampled['power_future'] = df_resampled['total_output_power_kW'].shift(-target_horizon)\n",
        "\n",
        "        # Drop rows with NaN values\n",
        "        self.df_clean = df_resampled.dropna()\n",
        "\n",
        "        # Add for later use in metaModel\n",
        "        self.target = self.df_clean['power_future']\n",
        "\n",
        "        # Add direct features if they exist in the dataframe\n",
        "        for feature in self.direct_features:\n",
        "            if feature in df_resampled.columns:\n",
        "                self.features.append(feature)\n",
        "                print(f\"Added direct feature: {feature}\")\n",
        "\n",
        "        # Print final feature list count\n",
        "        print(f\"Total features created: {len(self.features)}\")\n",
        "        print(f\"THES ARE THE COLUMNS: {[col for col in self.df_clean.columns]}\")\n",
        "\n",
        "    def analyze_features(self):\n",
        "        \"\"\"Analyze feature importance and relationships\"\"\"\n",
        "        X = self.df_clean[self.features]\n",
        "        y = self.df_clean['power_future']\n",
        "\n",
        "        # Correlation heatmap of top features with target\n",
        "        corr = pd.DataFrame(X.corrwith(y).sort_values(ascending=False)).reset_index()\n",
        "        corr.columns = ['Feature', 'Correlation']\n",
        "\n",
        "        plt.figure(figsize=(10, 12))\n",
        "        top_n = min(20, len(corr))\n",
        "        sns.barplot(x='Correlation', y='Feature', data=corr.head(top_n))\n",
        "        plt.title(f'Top {top_n} Feature Correlations with Target')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot relationship between top features and target\n",
        "        top_features = corr.head(5)['Feature'].tolist()\n",
        "\n",
        "        fig, axes = plt.subplots(len(top_features), 1, figsize=(12, 15))\n",
        "        for i, feature in enumerate(top_features):\n",
        "            axes[i].scatter(X[feature], y, alpha=0.4)\n",
        "            axes[i].set_xlabel(feature)\n",
        "            axes[i].set_ylabel('Future Power Output')\n",
        "            axes[i].set_title(f'{feature} vs Target')\n",
        "            axes[i].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return corr\n",
        "\n",
        "    def get_important_features(self, top_n=None, threshold=None):\n",
        "        \"\"\"\n",
        "        Get a list of important features based on trained model.\n",
        "\n",
        "        Parameters:\n",
        "        - top_n: Select the top N features by importance.\n",
        "        - threshold: Select features with importance above this threshold (e.g., 0.01).\n",
        "\n",
        "        Returns:\n",
        "        - List of selected feature names\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model must be trained first to compute feature importance.\")\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': self.features,\n",
        "            'Importance': self.model.feature_importances_\n",
        "        })\n",
        "\n",
        "        if top_n is not None:\n",
        "            selected = importance_df.sort_values('Importance', ascending=False).head(top_n)['Feature'].tolist()\n",
        "        elif threshold is not None:\n",
        "            selected = importance_df[importance_df['Importance'] >= threshold]['Feature'].tolist()\n",
        "        else:\n",
        "            raise ValueError(\"Provide either top_n or threshold.\")\n",
        "\n",
        "        return selected\n",
        "\n",
        "    def train_model(self, selected_features=None, param_grid=None):\n",
        "        \"\"\"Train XGBoost model with improved grid search\"\"\"\n",
        "        if selected_features:\n",
        "            used_features = [f for f in selected_features if f in self.features]\n",
        "        else:\n",
        "            used_features = self.features\n",
        "\n",
        "        X = self.df_clean[used_features]\n",
        "        y = self.df_clean['power_future']\n",
        "\n",
        "        # Train-test split with stratified temporal split\n",
        "        split_idx = int(len(self.df_clean) * 0.8)\n",
        "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "        self.X_train, self.X_test = X_train, X_test\n",
        "        self.y_train, self.y_test = y_train, y_test\n",
        "\n",
        "        # Scale features\n",
        "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
        "\n",
        "        # # Default param grid if none provided\n",
        "        # if param_grid is None:\n",
        "        #     param_grid = {\n",
        "        #         'n_estimators': [100, 200, 300],\n",
        "        #         'max_depth': [5, 7, 9],\n",
        "        #         'learning_rate': [0.01, 0.05, 0.1],\n",
        "        #         'subsample': [0.8, 0.9, 1.0],\n",
        "        #         'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "        #         'min_child_weight': [1, 3, 5],\n",
        "        #         'gamma': [0, 0.1, 0.2]\n",
        "        #     }\n",
        "\n",
        "        # # Grid search with cross-validation\n",
        "        # grid = RandomizedSearchCV(\n",
        "        #     XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "        #     param_distributions=param_grid,\n",
        "        #     n_iter=30,  # Try 20 random combinations\n",
        "        #     cv=3,\n",
        "        #     scoring='neg_mean_squared_error',\n",
        "        #     verbose=1,\n",
        "        #     n_jobs=-1\n",
        "        # )\n",
        "\n",
        "        # grid.fit(self.X_train_scaled, self.y_train)\n",
        "        # self.model = grid.best_estimator_\n",
        "\n",
        "        # print(f\"Best parameters: {grid.best_params_}\")\n",
        "\n",
        "        default_params = {\n",
        "            'n_estimators': 300,\n",
        "            'max_depth': 5,\n",
        "            'learning_rate': 0.01,\n",
        "            'subsample': 0.9,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'min_child_weight': 1,\n",
        "            'gamma': 0,\n",
        "            'objective': 'reg:quantileerror',\n",
        "            'quantile_alpha': 0.80,\n",
        "            'random_state': 42\n",
        "        }\n",
        "\n",
        "        # Initialize and fit the model\n",
        "        self.model = XGBRegressor(**default_params)\n",
        "        self.model.fit(self.X_train_scaled, self.y_train)\n",
        "\n",
        "        # Get feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': used_features,\n",
        "            'Importance': self.model.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(10, 12))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
        "        plt.title('Feature Importance (Top 20)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return feature_importance\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"Evaluate model with additional metrics\"\"\"\n",
        "        # Make predictions\n",
        "        y_pred_train = self.model.predict(self.X_train_scaled)\n",
        "        y_pred_test = self.model.predict(self.X_test_scaled)\n",
        "\n",
        "        # Clip negative predictions to zero\n",
        "        y_pred_train = np.clip(y_pred_train, a_min=0, a_max=None)\n",
        "        y_pred_test = np.clip(y_pred_test, a_min=0, a_max=None)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_metrics = {\n",
        "            'rmse': np.sqrt(mean_squared_error(self.y_train, y_pred_train)),\n",
        "            'mae': mean_absolute_error(self.y_train, y_pred_train),\n",
        "            'r2': r2_score(self.y_train, y_pred_train)\n",
        "        }\n",
        "\n",
        "        test_metrics = {\n",
        "            'rmse': np.sqrt(mean_squared_error(self.y_test, y_pred_test)),\n",
        "            'mae': mean_absolute_error(self.y_test, y_pred_test),\n",
        "            'r2': r2_score(self.y_test, y_pred_test)\n",
        "        }\n",
        "\n",
        "        # Calculate normalized metrics\n",
        "        y_mean = self.y_test.mean()\n",
        "        test_metrics['nrmse'] = test_metrics['rmse'] / y_mean\n",
        "        test_metrics['nmae'] = test_metrics['mae'] / y_mean\n",
        "\n",
        "        print(\"\\nTraining Metrics:\")\n",
        "        for k, v in train_metrics.items():\n",
        "            print(f\"{k.upper()}: {v:.4f}\")\n",
        "\n",
        "        print(\"\\nTest Metrics:\")\n",
        "        for k, v in test_metrics.items():\n",
        "            print(f\"{k.upper()}: {v:.4f}\")\n",
        "\n",
        "        self.metrics = test_metrics\n",
        "        return train_metrics, test_metrics\n",
        "\n",
        "    def plot_results(self):\n",
        "        \"\"\"Plot comprehensive evaluation results\"\"\"\n",
        "        y_pred = self.model.predict(self.X_test_scaled)\n",
        "        y_pred = np.clip(y_pred, a_min=0, a_max=None)\n",
        "\n",
        "        # Create a DataFrame with actual and predicted values\n",
        "        results_df = pd.DataFrame({\n",
        "            'Actual': self.y_test.values,\n",
        "            'Predicted': y_pred\n",
        "        }, index=self.y_test.index)\n",
        "\n",
        "        # Plot actual vs predicted time series\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        plt.plot(results_df.index, results_df['Actual'], label='Actual', color='blue', alpha=0.7)\n",
        "        plt.plot(results_df.index, results_df['Predicted'], label='Predicted', color='red', alpha=0.7)\n",
        "        plt.title('Solar Power Prediction - Actual vs Predicted')\n",
        "        plt.ylabel('Power Output (kW)')\n",
        "        plt.xlabel('Time')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot scatter of actual vs predicted\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.5)\n",
        "\n",
        "        # Add perfect prediction line\n",
        "        max_val = max(results_df['Actual'].max(), results_df['Predicted'].max())\n",
        "        plt.plot([0, max_val], [0, max_val], 'r--')\n",
        "\n",
        "        plt.title('Actual vs Predicted Power Output')\n",
        "        plt.xlabel('Actual Power (kW)')\n",
        "        plt.ylabel('Predicted Power (kW)')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot residuals\n",
        "        residuals = results_df['Actual'] - results_df['Predicted']\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.scatter(results_df['Predicted'], residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.title('Residuals vs Predicted')\n",
        "        plt.xlabel('Predicted Power (kW)')\n",
        "        plt.ylabel('Residual (Actual - Predicted)')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.hist(residuals, bins=30, alpha=0.7)\n",
        "        plt.title('Residual Distribution')\n",
        "        plt.xlabel('Residual Value')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot predictions for a sample period (2 weeks)\n",
        "        if len(results_df) > 14*24:  # If we have at least 2 weeks of data\n",
        "            sample_period = results_df.iloc[-14*24:]  # Last 2 weeks\n",
        "\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(sample_period.index, sample_period['Actual'], label='Actual', color='blue')\n",
        "            plt.plot(sample_period.index, sample_period['Predicted'], label='Predicted', color='red')\n",
        "            plt.title('2-Week Sample Period - Actual vs Predicted')\n",
        "            plt.ylabel('Power Output (kW)')\n",
        "            plt.xlabel('Time')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Display error over time of day\n",
        "        results_df['hour'] = results_df.index.hour\n",
        "        results_df['abs_error'] = abs(results_df['Actual'] - results_df['Predicted'])\n",
        "\n",
        "        hour_error = results_df.groupby('hour')['abs_error'].mean()\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        hour_error.plot(kind='bar')\n",
        "        plt.title('Mean Absolute Error by Hour of Day')\n",
        "        plt.xlabel('Hour of Day')\n",
        "        plt.ylabel('Mean Absolute Error')\n",
        "        plt.grid(True, axis='y')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def predict(self, data=None):\n",
        "        \"\"\"\n",
        "        Predict on the full dataset (no train-test split).\n",
        "        Useful when using this model as a feature generator for meta-models.\n",
        "\n",
        "        Parameters:\n",
        "        - data: optional, custom DataFrame to predict on.\n",
        "                If None, will use self.df_clean from previous preprocessing.\n",
        "\n",
        "        Returns:\n",
        "        - predictions: np.array of predicted values\n",
        "        - index: index of predictions (datetime or row index)\n",
        "        \"\"\"\n",
        "        # if self.model is None:\n",
        "        #     raise ValueError(\"Model has not been trained yet. Call train_model() first.\")\n",
        "\n",
        "        # if data is None:\n",
        "        #     if not hasattr(self, 'df_clean'):\n",
        "        #         raise ValueError(\"No preprocessed data available. Call preprocess_data() first.\")\n",
        "        #     X_full = self.df_clean[self.features]\n",
        "        #     index = self.df_clean.index\n",
        "        # else:\n",
        "        #     # Use passed data (must already have matching features)\n",
        "        #     missing = [feat for feat in self.features if feat not in data.columns]\n",
        "        #     if missing:\n",
        "        #         raise ValueError(f\"Missing required features: {missing}\")\n",
        "        if data is None:\n",
        "            data = self.df_clean\n",
        "\n",
        "        X_full = data[self.features]\n",
        "        index = data.index\n",
        "\n",
        "        X_scaled = self.scaler.transform(X_full)\n",
        "        predictions = self.model.predict(X_scaled)\n",
        "        predictions = np.clip(predictions, a_min=0, a_max=None)\n",
        "\n",
        "        return pd.Series(predictions, index=index, name=\"xgb_predictions\")\n",
        "\n",
        "\n",
        "    def test_for_leakage(self):\n",
        "        \"\"\"Leakage test: Train on shuffled targets and check performance\"\"\"\n",
        "\n",
        "        # Shuffle the target values (break any true pattern)\n",
        "        y_train_shuffled = np.random.permutation(self.y_train)\n",
        "\n",
        "        # Train a new XGBoost model on the same features but with shuffled targets\n",
        "        shuffled_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "        shuffled_model.fit(self.X_train_scaled, y_train_shuffled)\n",
        "\n",
        "        # Predict on the test set\n",
        "        y_pred_test = shuffled_model.predict(self.X_test_scaled)\n",
        "\n",
        "        # Evaluate performance on real test targets\n",
        "        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred_test))\n",
        "        mae = mean_absolute_error(self.y_test, y_pred_test)\n",
        "        r2 = r2_score(self.y_test, y_pred_test)\n",
        "\n",
        "        print(\"\\n=== Leakage Detection Test ===\")\n",
        "        print(\"Model trained on SHUFFLED target values:\")\n",
        "        print(f\"Test RMSE: {rmse:.4f}\")\n",
        "        print(f\"Test MAE: {mae:.4f}\")\n",
        "        print(f\"Test R²: {r2:.4f}\")\n",
        "\n",
        "        if r2 > 0.2:\n",
        "            print(\"⚠️ WARNING: Model is performing too well on shuffled data. Possible leakage!\")\n",
        "        else:\n",
        "            print(\"✅ No obvious leakage detected.\")\n",
        "\n",
        "        return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
        "\n",
        "\n",
        "    def run_pipeline(self, target_horizon=18, resample_freq='30T'):\n",
        "        \"\"\"Run the complete pipeline\"\"\"\n",
        "        print(\"Step 1: Preprocessing data...\")\n",
        "        self.preprocess_data(target_horizon=target_horizon, resample_freq=resample_freq)\n",
        "\n",
        "        print(\"\\nStep 2: Analyzing features...\")\n",
        "        feature_corr = self.analyze_features()\n",
        "\n",
        "        print(\"\\nstep 3: Get top features...\")\n",
        "        model.train_model()\n",
        "        top_features = model.get_important_features(top_n=30)\n",
        "\n",
        "        print(\"\\nTop 30 Features Used: \")\n",
        "        for i, feat in enumerate(top_features, start=1):\n",
        "            print(f\"{i}. {feat}\")\n",
        "\n",
        "        print(\"\\nStep 3: Training model...\")\n",
        "        feature_importance = self.train_model(selected_features = top_features)\n",
        "\n",
        "        print(\"\\nStep 4: Evaluating model...\")\n",
        "        train_metrics, test_metrics = self.evaluate_model()\n",
        "\n",
        "        print(\"\\nStep 5: Plotting results...\")\n",
        "        self.plot_results()\n",
        "\n",
        "        return {\n",
        "            'feature_correlation': feature_corr,\n",
        "            'feature_importance': feature_importance,\n",
        "            'metrics': test_metrics\n",
        "        }\n",
        "\n",
        "# Usage example\n",
        "# if __name__ == \"__main__\":\n",
        "    # Example of how to use the model with solar irradiance as a direct feature\n",
        "\n",
        "    # Option 1: Using default direct features (solar_irradiance)\n",
        "    # model = ImprovedXGBoostModel(data=df.reset_index())\n",
        "    # results = model.run_pipeline()\n",
        "    # model.train_model()\n",
        "    # model.test_for_leakage()\n",
        "    # model.test_for_leakage()\n",
        "    # # Option 2: Specify custom direct features\n",
        "    # model = ImprovedXGBoostModel(\n",
        "    #     file_path=\"your_solar_data.csv\",\n",
        "    #     direct_features=['solar_irradiance']\n",
        "    # )\n",
        "    # results = model.run_pipeline()\n"
      ],
      "metadata": {
        "id": "CqBl3kp_iaM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WC57vXkoiaKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8r0Ih7cZikdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "prgmlGxNil_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blender"
      ],
      "metadata": {
        "id": "I1kDGMggiqjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class blender:\n",
        "    def __init__(self, data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "        self.data = data\n",
        "        self.train_ratio = train_ratio\n",
        "        self.val_ratio = val_ratio\n",
        "        self.test_ratio = test_ratio\n",
        "        self.split_data()\n",
        "\n",
        "    def split_data(self):\n",
        "        total_len = len(self.data)\n",
        "        train_end = int(self.train_ratio * total_len)\n",
        "        val_end = train_end + int(self.val_ratio * total_len)\n",
        "\n",
        "        self.base_train_data = self.data.iloc[:train_end]\n",
        "        self.holdout_data = self.data.iloc[train_end:val_end]\n",
        "        self.test_data = self.data.iloc[val_end:]\n",
        "\n",
        "    def run_models(self, lstm_freq='60min', xgb_freq='1H', lstm_horizon=18, xgb_horizon=18):\n",
        "        # === Train LSTM ===\n",
        "        self.lstm_model = LSTMForecastModel(data=self.base_train_data)\n",
        "        self.lstm_model.preprocess_data(resample_freq=lstm_freq, target_horizon=lstm_horizon)\n",
        "        self.lstm_model.create_sequences()\n",
        "        self.lstm_model.build_model()\n",
        "        self.lstm_model.train_model()\n",
        "        self.lstm_model.evaluate_model()\n",
        "\n",
        "        # === LSTM Predictions ===\n",
        "        self.lstm_holdout = self.lstm_model.predict(\n",
        "            new_data=self.holdout_data,\n",
        "            resample_freq=lstm_freq,\n",
        "            target_horizon=lstm_horizon\n",
        "        )\n",
        "\n",
        "        self.y_lstm_test = self.lstm_model.predict(\n",
        "            new_data=self.test_data,\n",
        "            resample_freq=lstm_freq,\n",
        "            target_horizon=lstm_horizon\n",
        "        )\n",
        "\n",
        "        # === Train XGBoost ===\n",
        "        self.xgb_model = ImprovedXGBoostModel(data=self.base_train_data.reset_index())\n",
        "        self.xgb_model.preprocess_data(resample_freq=xgb_freq, target_horizon=xgb_horizon)\n",
        "        self.xgb_model.train_model()\n",
        "        self.xgb_model.evaluate_model()\n",
        "\n",
        "        # === XGBoost Predictions ===\n",
        "        # Create nwe model for holdout data\n",
        "        xgb_holdout_model = ImprovedXGBoostModel(data=self.holdout_data.reset_index())\n",
        "        xgb_holdout_model.preprocess_data(resample_freq=xgb_freq, target_horizon=xgb_horizon)\n",
        "        #reuse the trained model and scaler\n",
        "        xgb_holdout_model.model = self.xgb_model.model\n",
        "        xgb_holdout_model.scaler = self.xgb_model.scaler\n",
        "        # predictions\n",
        "        self.xgb_holdout = xgb_holdout_model.predict(data=xgb_holdout_model.df_clean)\n",
        "\n",
        "        # Create new model for the test data\n",
        "        xgb_test_model = ImprovedXGBoostModel(data=self.test_data.reset_index())\n",
        "        xgb_test_model.preprocess_data(resample_freq=xgb_freq, target_horizon=xgb_horizon)\n",
        "        xgb_test_model.model = self.xgb_model.model\n",
        "        xgb_test_model.scaler = self.xgb_model.scaler\n",
        "        # predict\n",
        "        self.xgb_test = xgb_test_model.predict(data=xgb_test_model.df_clean)\n",
        "\n",
        "        # === Align predictions and true values on holdout ===\n",
        "        y_true_holdout = xgb_holdout_model.target\n",
        "        common_index = y_true_holdout.index.intersection(self.xgb_holdout.index).intersection(self.lstm_holdout.index)\n",
        "\n",
        "        self.blend_df = pd.DataFrame({\n",
        "            'y_true': y_true_holdout.loc[common_index],\n",
        "            'lstm_pred': self.lstm_holdout.loc[common_index].values,\n",
        "            'xgb_pred': self.xgb_holdout.loc[common_index].values\n",
        "        }, index=common_index)\n",
        "\n",
        "        # Align test set predictions for blended model\n",
        "        y_true_test = xgb_test_model.target\n",
        "        common_test_index = y_true_test.index.intersection(self.y_lstm_test.index).intersection(self.xgb_test.index)\n",
        "\n",
        "        X_blend_df = pd.DataFrame({\n",
        "            'lstm_pred': self.y_lstm_test.loc[common_test_index].values,\n",
        "            'xgb_pred': self.xgb_test.loc[common_test_index].values\n",
        "        }, index=common_test_index)\n",
        "\n",
        "        y_true_aligned = y_true_test.loc[common_test_index]\n",
        "\n",
        "        self.X_blend_test = X_blend_df\n",
        "        self.y_blend_true = y_true_aligned\n",
        "\n",
        "    def train_blended_model(self):\n",
        "        from sklearn.linear_model import Ridge\n",
        "        from sklearn.linear_model import RidgeCV\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "        X_blend = self.blend_df[['lstm_pred', 'xgb_pred']]\n",
        "        y_blend = self.blend_df['y_true']\n",
        "\n",
        "        # scaler = StandardScaler()\n",
        "        # X_blend_scaled = scaler.fit_transform(X_blend)\n",
        "\n",
        "        self.blend_model = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0])\n",
        "        self.blend_model.fit(X_blend, y_blend)\n",
        "\n",
        "        # Align final test set inputs\n",
        "        X_blend_df = self.X_blend_test[['lstm_pred', 'xgb_pred']]\n",
        "        # X_blend_df_scaled = scaler.transform(X_blend_df)\n",
        "\n",
        "        self.blend_model_pred = pd.Series(\n",
        "            self.blend_model.predict(X_blend_df),\n",
        "            index=X_blend_df.index\n",
        "        )\n",
        "\n",
        "        # self.blend_model_pred = self.blend_model_pred[self.X_blend_test.index.get_indexer(features.index)]\n",
        "\n",
        "        return self.blend_model_pred\n",
        "\n",
        "    def _generate_time_features(self, index):\n",
        "        \"\"\"\n",
        "        Generates time-based features given a datetime index.\n",
        "\n",
        "        Parameters:\n",
        "        - index: pd.DatetimeIndex used to generate features.\n",
        "\n",
        "        Returns:\n",
        "        - DataFrame of time features.\n",
        "        \"\"\"\n",
        "        features = pd.DataFrame(index=index)\n",
        "        features['hour'] = index.hour\n",
        "        features['minute'] = index.minute\n",
        "        features['dayofweek'] = index.dayofweek  # 0=Monday, 6=Sunday\n",
        "        features['is_weekend'] = index.dayofweek >= 5\n",
        "        features['month'] = index.month\n",
        "        features['dayofyear'] = index.dayofyear\n",
        "        features['weekofyear'] = index.isocalendar().week.astype(int)\n",
        "        features['quarter'] = index.quarter\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _generate_residual_features(self, max_lag=5):\n",
        "        \"\"\"\n",
        "        Generate features for residual correction including time-based and lag features.\n",
        "\n",
        "        Parameters:\n",
        "        - max_lag: Number of lags to generate for predictions and residuals.\n",
        "\n",
        "        Returns:\n",
        "        - DataFrame of features aligned with the holdout set (blend_df).\n",
        "        \"\"\"\n",
        "        df = self.blend_df.copy()\n",
        "\n",
        "        # Step 1: Time features\n",
        "        time_features = self._generate_time_features(df.index)\n",
        "\n",
        "        # Step 2: Lag features for predictions and residuals\n",
        "        for lag in range(1, max_lag + 1):\n",
        "            time_features[f'lstm_pred_lag_{lag}'] = df['lstm_pred'].shift(lag)\n",
        "            time_features[f'xgb_pred_lag_{lag}'] = df['xgb_pred'].shift(lag)\n",
        "\n",
        "            # Optionally, lag the residuals themselves\n",
        "            # residuals = df['y_true'] - df[['lstm_pred', 'xgb_pred']].mean(axis=1)\n",
        "            # time_features[f'residual_lag_{lag}'] = residuals.shift(lag)\n",
        "\n",
        "\n",
        "        # Drop rows with NaNs from shifting\n",
        "        time_features.dropna(inplace=True)\n",
        "\n",
        "        # Also drop rows in blend_df to keep alignment\n",
        "        self.blend_df = self.blend_df.loc[time_features.index]\n",
        "\n",
        "        return time_features\n",
        "\n",
        "    def train_residual_model(self):\n",
        "        \"\"\"\n",
        "        Trains a model to predict the residual error from the meta-model,\n",
        "        using time-based features.\n",
        "        \"\"\"\n",
        "        from xgboost import XGBRegressor\n",
        "\n",
        "        # compute residuals\n",
        "        residual_target = self.blend_df['y_true'] - self.blend_model.predict(self.blend_df[['lstm_pred', 'xgb_pred']])\n",
        "\n",
        "        # create time-based features with blend_df index\n",
        "        time_features = self._generate_time_features(self.blend_df.index)\n",
        "        residual_features = self._generate_residual_features(max_lag=3)\n",
        "\n",
        "        print(self.holdout_data.columns)\n",
        "        original_features = self.holdout_data['WMS 01 irradiance']\n",
        "        original_features = original_features.loc[self.blend_df.index]  # align\n",
        "        residual_features = pd.concat([residual_features, original_features], axis=1)\n",
        "\n",
        "        # train residual model\n",
        "        self.residual_model = XGBRegressor()\n",
        "        # self.residual_model.fit(time_features, residual_target)\n",
        "        self.residual_model.fit(residual_features, residual_target.loc[residual_features.index])\n",
        "\n",
        "        # Save time_features for test correction\n",
        "        self.residual_train_features = time_features\n",
        "\n",
        "    def apply_residual_correction(self, max_lag=3):\n",
        "        \"\"\"\n",
        "        Applies residual correction on top of the meta-model predictions,\n",
        "        using time-based features from the test set.\n",
        "        \"\"\"\n",
        "\n",
        "        df_test = self.X_blend_test.copy()\n",
        "\n",
        "        # Step 1: generate time-based features\n",
        "        features = self._generate_time_features(df_test.index)\n",
        "\n",
        "        # Step 2: add lag features\n",
        "        for lag in range(1, max_lag + 1):\n",
        "            features[f'lstm_pred_lag_{lag}'] = df_test['lstm_pred'].shift(lag)\n",
        "            features[f'xgb_pred_lag_{lag}'] = df_test['xgb_pred'].shift(lag)\n",
        "\n",
        "        # Step 3: add external features\n",
        "        extra_features = self.test_data['WMS 01 irradiance']\n",
        "        extra_features = extra_features.loc[features.index]\n",
        "        features = pd.concat([features, extra_features], axis=1)\n",
        "\n",
        "        # Step 4: Drop NaNs from shifting\n",
        "        features.dropna(inplace=True)\n",
        "        self.X_blend_test = self.X_blend_test.loc[features.index]\n",
        "        self.y_blend_true = self.y_blend_true.loc[features.index]\n",
        "        self.blend_model_pred = self.blend_model_pred[features.index]\n",
        "\n",
        "        # Step 5: predict residuals & correct\n",
        "        residual_pred = self.residual_model.predict(features)\n",
        "        corrected_preds = self.blend_model_pred + residual_pred\n",
        "\n",
        "        # Step 6: clip negative values\n",
        "        corrected_preds = np.clip(corrected_preds, 0, None)\n",
        "\n",
        "        # Step 7: optionally force nighttime values to 0\n",
        "        # night_mask = self.test_data.loc[corrected_preds.index, 'is_daytime'] == 0\n",
        "        # corrected_preds[night_mask] = 0\n",
        "\n",
        "        # Update prediction\n",
        "        self.blend_model_pred = corrected_preds\n",
        "\n",
        "        return corrected_preds\n",
        "\n",
        "\n",
        "    def evaluate_blend_model(self, y_test_true):\n",
        "        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "        # align actual with prediction index\n",
        "        y_true = y_test_true.loc[self.X_blend_test.index]\n",
        "        y_pred = self.blend_model_pred\n",
        "\n",
        "        # metrics\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "        print(f\"Meta-Model Evaluation:\\nMAE: {mae:.3f} | RMSE: {rmse:.3f} | R²: {r2:.3f}\")\n",
        "\n",
        "        # Plot actual vs predicted\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(y_true.index, y_true.values, label='Actual', linewidth=2)\n",
        "        plt.plot(y_true.index, y_pred, label='Meta Predicted', linestyle='--')\n",
        "        plt.title(\"Meta Model: Actual vs Predicted\")\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Power Output (kW)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return mae, rmse, r2"
      ],
      "metadata": {
        "id": "T-vxAHHJis2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blender = blender(df)\n",
        "blender.run_models(lstm_freq='15min', xgb_freq='15min', lstm_horizon=18, xgb_horizon=18)"
      ],
      "metadata": {
        "id": "gU4hcxkZitmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blender.train_blended_model()\n",
        "blender.evaluate_blend_model(blender.y_blend_true)"
      ],
      "metadata": {
        "id": "Ru2aVJ6JiwjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blender.train_blended_model()\n",
        "blender.train_residual_model()\n",
        "blender.apply_residual_correction()\n",
        "blender.evaluate_blend_model(blender.y_blend_true)"
      ],
      "metadata": {
        "id": "0fVysC8DiwcP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}